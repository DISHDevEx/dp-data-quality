AWSTemplateFormatVersion: 2010-09-09
Description: deploy yaml and python files with aws cloudformation for s3 to s3 validation.

########################################################################################
# Copyright 2021 DISH Wireless Network Data Platform. or its affiliates.               #
# All Rights Reserved.                                                                 #
# Permission is hereby granted, free of charge, to any person obtaining a copy of this #
# software and associated documentation files (the "Software"), to deal in the Software#
# without restriction, including without limitation the rights to use, copy, modify,   #
# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to   #
# permit persons to whom the Software is furnished to do so.                           #
#                                                                                      #
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,  #
# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A        #
# PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT   #
# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION    #
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE       #
# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.                               #
########################################################################################

#!/usr/bin/env python3  
##--------------------------------------------------------------------
## File    :   s3validation.yaml
## Time    :   2022/12/05 12:02:01
## Author  :   Zheng Liu
## Version :   1.0
## Desc    :   deploy s3, lambda, glue job and sns with appropriate configurations in term of automated s3 validation architecture. this is a demo

#---------------------------Version History---------------------------
#SrNo    DateModified    ModifiedBy   Description
#1       2022/12/05      Zheng        Initial Version
#2       2022/12/22      Zheng        Changed unquote_plus to unquote, so "+" will not be converted to another value
#3       2023/01/25      Zheng        Able to create roles for services
##--------------------------------------------------------------------

# Stackname should be Target S3 name + '---<bucketprefix>(letters and numbers only)'
# Initally deploy all new resources without Target S3
# Upload s3_to_s3_validation_script.py in target validation folder under target S3
# Make sure target folder (TargetValidationPrefix) is not the same as trigger folder(TargetTriggerPrefix)
# Then import Target S3 into stack created above
# Finally update the stack above by adding trigger from Target S3 to LambdaAsTrigger
# All activities are in same account
# The only valid file name to trigger the validation is s3_to_s3_validation.csv

Parameters:

  AmazonS3FullAccessPolicy:
    Type: String
    Description: Need read from S3
    Default: arn:aws:iam::aws:policy/AmazonS3FullAccess

  CloudWatchLogsFullAccessPolicy:
    Type: String
    Description: Need cloudwatch log to let S3 trigger Lambda
    Default: arn:aws:iam::aws:policy/CloudWatchLogsFullAccess

  AWSGlueServiceRolePolicy:
    Type: String
    Description: Saving execution record in logs
    Default: arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole

  AmazonSNSFullAccessPolicy:
    Type: String
    Description: Need to trigger SNS
    Default: arn:aws:iam::aws:policy/AmazonSNSFullAccess

  PermissionBoundary:
    Type: String
    MinLength: '1'
    Description: permission boundary of the current role
    Default: TaaSAdminDev_Permission_Boundary
  
  TargetS3Name:
    Type: String
    MinLength: '1'
    Description: Target S3 bucket name
    Default: "Please Enter"

  TargetValidationPrefix:
    Type: String
    MinLength: '1'
    Description: Target validation prefix in target S3 bucket
    Default: "Please Enter"

  TargetTriggerPrefix:
    Type: String
    MinLength: '1'
    Description: Target trigger prefix in target S3 bucket
    Default: "Please Enter"

  ValidationScriptFileNameInTargetS3:
    Type: String
    MinLength: '1'
    Description: Name of the Code Object under Target S3 Bucket in Target validation folder
    Default: s3_to_s3_validation_script.py
    
  SNSEndpoints:
    Type: String
    MinLength: '1'
    Description: Name of the subscriber of the SNS topic
    Default: metadq@dish.com

  SNSEndpointProtocol:
    Type: String
    MinLength: '1'
    Description: Name of the protocol to notice subscriber of the SNS topic
    Default: email

  GlueJobNumberOfWorkers:
    Type: Number
    Description: Number of the Glue Job workers
    Default: 149


Resources:

  TheSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      Subscription:
        - Endpoint: !Ref SNSEndpoints
          Protocol: !Ref SNSEndpointProtocol
      TopicName: !Join ["", !Split [".", !Ref TargetS3Name]]

  TheGlueJobRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}GlueJobRole
      PermissionsBoundary: !Join 
        - ''
        - - "arn:aws:iam::"
          - !Ref "AWS::AccountId"
          - :policy/
          - !Ref PermissionBoundary     
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - !Ref AmazonS3FullAccessPolicy
        - !Ref AmazonSNSFullAccessPolicy
        - !Ref AWSGlueServiceRolePolicy

  TheGlueJob:
    Type: AWS::Glue::Job
    Properties: 
      Command: 
        Name: glueetl
        ScriptLocation: !Sub s3://${TargetS3Name}/${TargetValidationPrefix}/${ValidationScriptFileNameInTargetS3}
      Description: Triggered by S3 new object and do s3 to s3 validation and then output to SNS and S3
      GlueVersion: '3.0'
      MaxRetries: 0
      Name: !Join
        - '/'
        - - !Ref TargetS3Name
          - !Ref TargetValidationPrefix
      Role: !GetAtt TheGlueJobRole.Arn
      Timeout: 300
      WorkerType: G.2X
      NumberOfWorkers: !Ref GlueJobNumberOfWorkers
      DefaultArguments:
        "--job-bookmark-option": "job-bookmark-enable"
        "--Generate job insights": True


  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::StackName}LambdaRole
      PermissionsBoundary: !Join 
        - ''
        - - "arn:aws:iam::"
          - !Ref "AWS::AccountId"
          - :policy/
          - !Ref PermissionBoundary     
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Path: /
      ManagedPolicyArns:
        - !Ref AmazonS3FullAccessPolicy
        - !Ref CloudWatchLogsFullAccessPolicy
        - !Ref AWSGlueServiceRolePolicy

  LambdaAsTrigger:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::StackName}
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          glue_job_name: !Join
            - '/'
            - - !Ref TargetS3Name
              - !Ref TargetValidationPrefix
      Code:
        ZipFile: |
          import boto3
          import os
          import urllib.parse

          print('Loading function')
          
          # Prepare api calls for s3 and glue
          s3 = boto3.client('s3')
          glueclient = boto3.client('glue')
          # Prepare the glue job to call
          glueJobName = os.environ['glue_job_name']

          def lambda_handler(event, context):

              # Get the bucket name and key path of the object from the event
              bucket = event['Records'][0]['s3']['bucket']['name']
              path = urllib.parse.unquote(event['Records'][0]['s3']['object']['key'], encoding='utf-8')

              args = {
                  "--s3_bucket": bucket,
                  "--s3_path": path
              }

              print('args')
              print(args)

              # Try to see if the object is there
              try:
                  response_s3 = s3.get_object(Bucket=bucket, Key=path)
                  print("CONTENT TYPE: " + response_s3['ContentType'])
              except Exception as e:
                  print(e)
                  print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(path, bucket))
                  raise e
              
              # Pass the bucket name and path to glue job for validation
              try:
                  response_glue = glueclient.start_job_run(JobName = glueJobName, Arguments = args)
                  return response_glue
              except Exception as e:
                  print(e)
                  print('can not start glue job')
                  raise e

      Timeout: 250
      Runtime: python3.9

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LambdaAsTrigger.Arn
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceAccount: !Sub ${AWS::AccountId}

